{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy6m1Mle7AK+AJOCOCcpOK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ejini6969/Text-Analytics/blob/main/Group_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before doing text sentiment and analysis, open the file and output its content \n",
        "file = open(\"/content/Text Corpus.txt\")\n",
        "texts = file.read()\n",
        "print(texts)\n",
        "file.close()  # Close the file to prevent data loss and corruption"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2lujdbFa1Du",
        "outputId": "80f3e082-ef64-4927-da2a-aad61e3c6825"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> He read a book </s>\n",
            "<s> I read a different book </s>\n",
            "<s> He read a book my Danielle </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Implement and report the respective sentence probabilities in python using both unigram and bigram language models. "
      ],
      "metadata": {
        "id": "GTIZy2-Tt7RB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOOTHED UNIGRAM CALCULATION WITHOUT MODEL LIBRARY IMPORT\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# find the frequency of each token in the text corpus\n",
        "dict_frequency = {}\n",
        "unigram_smooth = {}\n",
        "text = texts.replace(\"<s>\", \"\").replace(\"</s>\", \"\").lower() # remove paddings and convert text corpus to lowercase\n",
        "tokens = word_tokenize(text) \n",
        "print(tokens)\n",
        "for t in tokens:\n",
        "    if t not in dict_frequency: dict_frequency[t] = 1\n",
        "    else:                       dict_frequency[t] += 1\n",
        "print(dict_frequency)\n",
        "# compute the unigram probability of each token in text corpus\n",
        "uniq = dict_frequency.keys()               # list out unique words in text corpus\n",
        "V = len(uniq)                              # number of unique words in text corpus\n",
        "total_words = sum(dict_frequency.values()) # number of total words in text corpus\n",
        "denominator = V + total_words + 1\n",
        "for k, v in dict_frequency.items():\n",
        "  prob = (v + 1) / denominator\n",
        "  unigram_smooth[k] = prob\n",
        "print(unigram_smooth)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7GqDs9It_Ow",
        "outputId": "aebc9f7f-b499-4feb-be52-095eeb4bc07e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['he', 'read', 'a', 'book', 'i', 'read', 'a', 'different', 'book', 'he', 'read', 'a', 'book', 'my', 'danielle']\n",
            "{'he': 2, 'read': 3, 'a': 3, 'book': 3, 'i': 1, 'different': 1, 'my': 1, 'danielle': 1}\n",
            "{'he': 0.125, 'read': 0.16666666666666666, 'a': 0.16666666666666666, 'book': 0.16666666666666666, 'i': 0.08333333333333333, 'different': 0.08333333333333333, 'my': 0.08333333333333333, 'danielle': 0.08333333333333333}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOOTHED UNIGRAM CALCULATION WITH MODEL LIBRARY IMPORT\n",
        "\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "\n",
        "n, total_words, sets = 1, len(tokens), set(tokens)\n",
        "V = len(sets)  # unique words in corpus \n",
        "denominator = total_words + V + 1\n",
        "unigram_smooth = {}\n",
        "\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, [tokens]) # unigram for n = 1\n",
        "model = MLE(n) # unigrams maximum likelihood estimation model\n",
        "model.fit(train_data, padded_sents)\n",
        "for x in sets:\n",
        "  prob = (model.counts[x] + 1) / denominator\n",
        "  unigram_smooth[x] = prob\n",
        "print(unigram_smooth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vQ2hHXIlDtr",
        "outputId": "d6ff6317-6d42-45d7-9209-6696bdc0378a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'danielle': 0.08333333333333333, 'a': 0.16666666666666666, 'book': 0.16666666666666666, 'he': 0.125, 'my': 0.08333333333333333, 'i': 0.08333333333333333, 'different': 0.08333333333333333, 'read': 0.16666666666666666}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute sentence probability based on smoothed unigram model above\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = text.split(\"\\n\")\n",
        "for sent in sentence:\n",
        "  sent = word_tokenize(sent)\n",
        "  probability = 1\n",
        "  for word in sent:\n",
        "    probability *= unigram_smooth[word]\n",
        "  print(f'P(\"{\" \".join(sent)}\") = {probability}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5BPm7vWy5oS",
        "outputId": "10534b76-f172-4ffc-a5b5-428d74d5efe5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(\"he read a book\") = 0.0005787037037037037\n",
            "P(\"i read a different book\") = 3.215020576131687e-05\n",
            "P(\"he read a book my danielle\") = 4.0187757201646085e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOOTHED BIGRAM CALCULATION WITHOUT MODEL LIBRARY IMPORT\n"
      ],
      "metadata": {
        "id": "DSKydNZc3srD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOOTHED BIGRAM CALCULATION WITH MODEL LIBRARY IMPORT\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import flatten, bigrams\n",
        "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "\n",
        "text2 = texts.replace(\"<s>\", \"\").replace(\"</s>\", \"\").lower() # remove paddings and convert text corpus to lowercase\n",
        "tokens = word_tokenize(text2)\n",
        "sets = set(tokens)                # list out unique words in text corpus\n",
        "V = len(sets)                     # number of unique words in text corpus\n",
        "sentences = text2.split(\"\\n\")     # split each line to form sentences \n",
        "bigram_lst = []                   # store all bigrams from padded sentences\n",
        "bigram_smooth, text_counter = {}, {}\n",
        "for sent in sentences:\n",
        "  tokens = word_tokenize(sent)\n",
        "  padded_lst = list(pad_both_ends(tokens, n = 2)) # add start and end paddings to each sentence\n",
        "  padded_bigram = list(bigrams(padded_lst)) # generate bigrams for each sentence\n",
        "  bigram_lst.append(padded_bigram)\n",
        "print(bigram_lst)\n",
        "\n",
        "\"\"\"padded_bigram = list(bigrams(padded_lst)) # generate bigrams for each sentence\n",
        "  bigram_lst.append(padded_bigram)\n",
        "\n",
        "  n = 2\n",
        "  train_data, padded_sents = padded_everygram_pipeline(n, [padded_lst]) # bigram for n = 2\n",
        "  model = MLE(n) # bigrams maximum likelihood estimation model\n",
        "  model.fit(train_data, padded_sents)\n",
        "\n",
        "  # compute probability of each bigram in text corpus\n",
        "  for x, y in padded_bigram:                \n",
        "    key = f'P({y} | {x})'\n",
        "    numerator = model.counts[[x]][y] + 1\n",
        "    prev_count = model.counts[x]\n",
        "    denominator = prev_count + V + 1\n",
        "    print(prev_count, numerator, denominator)\n",
        "    prob = numerator / denominator\n",
        "    bigram_smooth[key] = prob\n",
        "print(bigram_smooth, \"\\n\")\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "9FeDidyo38bP",
        "outputId": "a2fb32cd-6a43-46c3-84c1-34c74a40135a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['<s>', 'he', 'read', 'a', 'book', '</s>'], ['<s>', 'i', 'read', 'a', 'different', 'book', '</s>'], ['<s>', 'he', 'read', 'a', 'book', 'my', 'danielle', '</s>']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'padded_bigram = list(bigrams(padded_lst)) # generate bigrams for each sentence\\n  bigram_lst.append(padded_bigram)\\n\\n  n = 2\\n  train_data, padded_sents = padded_everygram_pipeline(n, [padded_lst]) # bigram for n = 2\\n  model = MLE(n) # bigrams maximum likelihood estimation model\\n  model.fit(train_data, padded_sents)\\n  padded_bigram = list(bigrams(padded_lst)) # generate bigrams for each sentence\\n  print(padded_bigram, \"\\n\")\\n  # compute probability of each bigram in text corpus\\n  for x, y in padded_bigram:                \\n    key = f\\'P({y} | {x})\\'\\n    numerator = model.counts[[x]][y] + 1\\n    prev_count = model.counts[x]\\n    denominator = prev_count + V + 1\\n    print(prev_count, numerator, denominator)\\n    prob = numerator / denominator\\n    bigram_smooth[key] = prob\\nprint(bigram_smooth, \"\\n\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    }
  ]
}